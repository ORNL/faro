{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faro\n",
    "import faro.proto.proto_types as pt\n",
    "import os\n",
    "import pyvision as pv\n",
    "import cv2\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaroDetectionOptions(object):\n",
    "    #user could change these variable : User-defined variables\n",
    "    def __init__(self):\n",
    "        #root file\n",
    "        self.results_root_path = '/home/nv5/Research/FaceRecognition/faro/Notebooks/results_dlib'\n",
    "        self.csvfiles_log = 'csvfiles_log'\n",
    "        #Directory to save images with bounding boxes\n",
    "        self.detect_log = 'detect_log'\n",
    "        #Directory to save cropped face images (128*128)\n",
    "        self.face_log = 'face_log'\n",
    "        #Set to True if you want to save only the detection with \n",
    "        #the highest detection score (confidence)\n",
    "        self.best = False\n",
    "        #Detection Thresh - Retinaface uses 0.5 in their examples\n",
    "        self.detect_thresh = 0.5\n",
    "        #Set min size. If the bouding box of the face detected is\n",
    "        #less that min_size then it will be discarded.\n",
    "        self.min_size = 0\n",
    "        #Set the maximum number of images it can process\n",
    "        self.max_images = None\n",
    "        self.max_size = None\n",
    "\n",
    "class FaroClientConnectionOptions(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.max_async = faro.DEFAULT_MAX_ASYNC\n",
    "        self.max_message_size = faro.DEFAULT_MAX_MESSAGE_SIZE\n",
    "        self.detect_port = faro.DEFAULT_PORT\n",
    "        self.rec_port = faro.DEFAULT_PORT\n",
    "        self.verbose = True\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method FaceClient.status of <faro.FaceClient.FaceClient object at 0x7f7d11ce97f0>>\n",
      "<class 'faro.proto.face_service_pb2.FaceServiceInfo'> status: READY\n",
      "detection_support: true\n",
      "extract_support: true\n",
      "score_support: true\n",
      "score_type: L2\n",
      "match_threshold: 0.6000000238418579\n",
      "algorithm: \"DLIB_19.18.0\"\n",
      "\n",
      "Connection to FaRO service established. [ algorithm: DLIB_19.18.0 ]\n"
     ]
    }
   ],
   "source": [
    "client_options = FaroClientConnectionOptions()\n",
    "face_client = faro.FaceClient(client_options)\n",
    "is_ready,status = face_client.status(verbose=client_options.verbose)\n",
    "if not is_ready:\n",
    "    print(\"ERROR: the FaRO service is not ready.\")\n",
    "    print(status)\n",
    "    exit(-1)\n",
    "else:\n",
    "    if client_options.verbose:\n",
    "        print('Connection to FaRO service established. [ algorithm: %s ]'%(status.algorithm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the input file is a image or video\n",
    "def getFilesToProcess(args):\n",
    "    images = []\n",
    "    videos = []\n",
    "    for each in args:\n",
    "        print(each)\n",
    "        if os.path.isfile(each) and pv.isImage(each):\n",
    "            images.append(each)\n",
    "        elif os.path.isfile(each) and pv.isVideo(each):\n",
    "            videos.append(each)\n",
    "        elif os.path.isdir(each):\n",
    "            for path,dirs,files in os.walk(each):\n",
    "                for filename in files:\n",
    "                    #print(filename)\n",
    "                    filepath = os.path.join(path,filename)\n",
    "                    if os.path.isfile(filepath) and pv.isImage(filepath):\n",
    "                        images.append(filepath)\n",
    "                    if os.path.isfile(filepath) and pv.isVideo(filepath):\n",
    "                        videos.append(filepath)\n",
    "        else :\n",
    "            raise ValueError(\"Cannot determine filetype:\"+each)\n",
    "\n",
    "    print(\"Found %d images and %d videos.\"%(len(images),len(videos)))\n",
    "    return images, videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImage(im_interest, doptions):\n",
    "    \"\"\"\n",
    "    If the doptions.max_size is defined then \n",
    "    if the image width or height is greater then\n",
    "    doptions.max then it will be resized with \n",
    "    w and h equal to doptions.max\n",
    "    \"\"\"\n",
    "    scale = 1.0\n",
    "    while max(*im_interest.shape[:2]) > doptions.max_size:\n",
    "        if max(*im_interest.shape[:2]) > 2*doptions.max_size:\n",
    "            im = cv2.pyrDown(im)\n",
    "            scale *= 0.5\n",
    "        else:\n",
    "            w,h = im.shape[:2]\n",
    "            s = doptions.max_size/max(w,h)\n",
    "            scale *= s\n",
    "            w = int(s*w)\n",
    "            h = int(s*h)\n",
    "            im = cv2.resize(im,(w,h))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nv5/Research/FaceRecognition/faro/bin/data/\n",
      "Found 10 images and 1 videos.\n"
     ]
    }
   ],
   "source": [
    "detection_options = FaroDetectionOptions()\n",
    "if not os.path.isdir(detection_options.results_root_path):\n",
    "    os.makedirs(detection_options.results_root_path)\n",
    "#define data path \n",
    "data_dir = ['/home/nv5/Research/FaceRecognition/faro/bin/data/']\n",
    "#get input file to process\n",
    "image_list, video_list = getFilesToProcess(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_detections(each):\n",
    "    \"\"\"\n",
    "    Detections returned by the detector are written into a csvfile\n",
    "    Images annotated with bouding box are saved to a directory if options.detect_log is True\n",
    "    Face region is extracted and resized to 128*128*3 and save to a directory of options.face_log is True\n",
    "    \"\"\"\n",
    "    im, results, options, media_type = each\n",
    "    if results.done():\n",
    "        recs = results.result().face_records\n",
    "        i = 0\n",
    "        dimg = None\n",
    "        csv_file = None\n",
    "        for idx, face in enumerate(recs):\n",
    "            base_name, ext = os.path.splitext(os.path.basename(face.source))\n",
    "            # Filter faces based on min size\n",
    "            size = min(face.detection.location.width,face.detection.location.height)\n",
    "            if size < options.min_size:\n",
    "                continue\n",
    "    \n",
    "            # Process Detections\n",
    "            if csv_file is None:\n",
    "                image_detection_file = open(os.path.join(options.results_root_path, \n",
    "                                                         options.csvfiles_log, media_type, base_name + '.csv'), 'w')\n",
    "                file_identifier = csv.writer(image_detection_file)\n",
    "                \n",
    "                csv_header = ['source','frame','detect_id','type','score','x','y','w','h']\n",
    "                if len(face.landmarks) > 0:\n",
    "                    \n",
    "                    for each_lpt in face.landmarks:\n",
    "                        pt_id_label = each_lpt.landmark_id\n",
    "                        xpt_label = pt_id_label + '_x'\n",
    "                        ypt_label = pt_id_label + '_y'\n",
    "                        csv_header.append(pt_id_label)\n",
    "                        csv_header.append(xpt_label)\n",
    "                        csv_header.append(ypt_label)\n",
    "                    \n",
    "                file_identifier.writerow(csv_header)\n",
    "                csv_file = -1\n",
    "                        \n",
    "                                 \n",
    "            csv_eachline = [face.source,\n",
    "                            face.frame,\n",
    "                            i,\n",
    "                            face.detection.detection_class,\n",
    "                            face.detection.score,\n",
    "                            face.detection.location.x,\n",
    "                            face.detection.location.y,\n",
    "                            face.detection.location.width,\n",
    "                            face.detection.location.height]\n",
    "            \n",
    "            if len(face.landmarks) > 0:\n",
    "                for each_lpt in face.landmarks:\n",
    "                    pt_id_label = each_lpt.landmark_id\n",
    "                    xpt_label = each_lpt.location.x\n",
    "                    ypt_label = each_lpt.location.y\n",
    "                    csv_eachline.append(pt_id_label)\n",
    "                    csv_eachline.append(xpt_label)\n",
    "                    csv_eachline.append(ypt_label)\n",
    "                    \n",
    "            file_identifier.writerow(csv_eachline)\n",
    "            \n",
    "                \n",
    "            if options.detect_log:\n",
    "                detect_log_dir = os.path.join(options.results_root_path, options.detect_log, media_type)\n",
    "                if not os.path.exists(detect_log_dir):\n",
    "                    os.makedirs(detect_log_dir, exist_ok=True)\n",
    "                rect = pt.rect_proto2pv(face.detection.location)\n",
    "                if dimg is None:\n",
    "                    dimg = pv.Image(im[:,:,::-1])\n",
    "                dimg.annotateThickRect(rect)\n",
    "                dimg.annotateLabel(pv.Point(rect.x+5,rect.y+5),face.detection.detection_class)\n",
    "                dimg.annotateLabel(pv.Point(rect.x+5,rect.y+20),\n",
    "                                    \"Score: %0.4f\"%(face.detection.score,), color='yellow')\n",
    "                if len(face.landmarks) > 0:\n",
    "                    for each_lmark in face.landmarks:\n",
    "                            dimg.annotateCircle(pv.Point(each_lmark.location.x, each_lmark.location.y), \n",
    "                                                radius=3, color = 'green', fill='green')\n",
    "                \n",
    "            \n",
    "            if options.face_log:\n",
    "                face_log_dir = os.path.join(options.results_root_path, options.face_log, media_type)\n",
    "                if not os.path.exists(face_log_dir):\n",
    "                    os.makedirs(face_log_dir, exist_ok=True)\n",
    "                #print(face.detection.location)\n",
    "                \n",
    "                rect = pt.rect_proto2pv(face.detection.location)\n",
    "                rect = rect.rescale(1.5)\n",
    "                affine = pv.AffineFromRect(rect,(128,128))\n",
    "    \n",
    "                try:\n",
    "                    pvim = pv.Image(im[:,:,::-1])\n",
    "                    view = affine(pvim)\n",
    "                    if len(face.landmarks) > 0:\n",
    "                        for each_lmark in face.landmarks:\n",
    "                            transformed_lmarks = affine(pv.Point(each_lmark.location.x, each_lmark.location.y))\n",
    "                            view.annotateCircle(transformed_lmarks, radius=3, color = 'green', fill='green')\n",
    "  \n",
    "                    base_name, ext = os.path.splitext(os.path.basename(face.source))\n",
    "                    out_path = os.path.join(face_log_dir,\n",
    "                                            os.path.basename(base_name)+'_face_%03d'%(face.detection.detection_id,)+ext)\n",
    "                    \n",
    "                    view.asAnnotated().save(out_path)\n",
    "                    print('Saving face:',out_path)\n",
    "                except:\n",
    "                    print(\"WARNING: Image not processed correctly:\",face.source)\n",
    "            i += 1\n",
    "        \n",
    "        if options.detect_log:\n",
    "            dimg.asAnnotated().save(os.path.join(detect_log_dir,\n",
    "                                                     os.path.basename(base_name) + ext))\n",
    "        image_detection_file.close()\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(ilist, doptions):\n",
    "    \"\"\"\n",
    "    Looping over images in the dataset and making client calls to the service\n",
    "    \"\"\"\n",
    "    image_count = 0\n",
    "    detect_queue = []\n",
    "    start_time = time.time()\n",
    "    for each_img in ilist:\n",
    "        im = cv2.imread(each_img)\n",
    "        \n",
    "        if im is None:\n",
    "            continue\n",
    "        # convert BGR to RGB\n",
    "        im = im[:,:,::-1]\n",
    "        if doptions.max_size is not None:\n",
    "            im = preprocessImage(im, doptions)\n",
    "        results = face_client.detect(im, best=doptions.best, \n",
    "                             threshold=doptions.detect_thresh, \n",
    "                             min_size=doptions.min_size,\n",
    "                             run_async=True, \n",
    "                             source=each_img, \n",
    "                             frame=-1)\n",
    "        detect_queue.append([im, results, doptions, 'image'])\n",
    "        detect_queue = list(filter(process_image_detections, detect_queue))\n",
    "        image_count += 1\n",
    "        if doptions.max_images is not None and image_count >= options.max_images:\n",
    "            break \n",
    "            \n",
    "    while len(detect_queue):\n",
    "        detect_queue = list(filter(process_image_detections,detect_queue))\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    print(\"Processed %d images in %0.3f seconds: %f images/second\"%(image_count,end_time - start_time,\n",
    "                                                                    image_count/(end_time - start_time)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "_Rendezvous",
     "evalue": "<_Rendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = \"Exception calling application: local variable 'detection_threshold' referenced before assignment\"\n\tdebug_error_string = \"{\"created\":\"@1592914460.757749421\",\"description\":\"Error received from peer ipv4:0.0.0.0:50030\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1055,\"grpc_message\":\"Exception calling application: local variable 'detection_threshold' referenced before assignment\",\"grpc_status\":2}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_Rendezvous\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-4222291770ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_root_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsvfiles_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_root_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsvfiles_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-76-7aa85eed3ae9>\u001b[0m in \u001b[0;36mprocess_images\u001b[0;34m(ilist, doptions)\u001b[0m\n\u001b[1;32m     22\u001b[0m                              frame=-1)\n\u001b[1;32m     23\u001b[0m         \u001b[0mdetect_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mdetect_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_image_detections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetect_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mimage_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_images\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimage_count\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-c61898d072a1>\u001b[0m in \u001b[0;36mprocess_image_detections\u001b[0;34m(each)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedia_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mrecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/faro_fr/lib/python3.5/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFutureCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_Rendezvous\u001b[0m: <_Rendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = \"Exception calling application: local variable 'detection_threshold' referenced before assignment\"\n\tdebug_error_string = \"{\"created\":\"@1592914460.757749421\",\"description\":\"Error received from peer ipv4:0.0.0.0:50030\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1055,\"grpc_message\":\"Exception calling application: local variable 'detection_threshold' referenced before assignment\",\"grpc_status\":2}\"\n>"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(os.path.join(detection_options.results_root_path, detection_options.csvfiles_log, 'image')):\n",
    "    os.makedirs(os.path.join(detection_options.results_root_path, detection_options.csvfiles_log, 'image'))\n",
    "process_images(image_list, detection_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View one output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "if detection_options.detect_log is not None:\n",
    "    from glob import glob\n",
    "    idir = os.path.join(detection_options.results_root_path, detection_options.detect_log, 'image')\n",
    "    bbox_imgs = glob(os.path.join(idir,'*'))\n",
    "    for each_img_path in bbox_imgs:\n",
    "        print(each_img_path)\n",
    "        img = mpimg.imread(each_img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if detection_options.face_log is not None:\n",
    "    from glob import glob\n",
    "    bbox_imgs =  glob(os.path.join(detection_options.results_root_path, detection_options.face_log, 'image','*'))\n",
    "    img_path = None\n",
    "    for each_img_path in bbox_imgs:\n",
    "        print(each_img_path)\n",
    "        img = mpimg.imread(each_img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_detections(each):\n",
    "    \"\"\"\n",
    "    Detections returned by the detector are written into a csvfile\n",
    "    Frames annotated with bouding box are saved to a directory if options.detect_log is True\n",
    "    Face region is extracted and resized to 128*128*3 and save to a directory of options.face_log is True\n",
    "    Only frames with detections are saved\n",
    "    \"\"\"\n",
    "    global video_header_flag\n",
    "    im, results, options, file_identifier, media_type = each\n",
    "    if results.done():\n",
    "        recs = results.result().face_records\n",
    "        i = 0\n",
    "        detect_log_dir = None\n",
    "        frame_id = None\n",
    "        dimg = None\n",
    "        for idx, face in enumerate(recs):\n",
    "            base_name, ext = os.path.splitext(os.path.basename(face.source))\n",
    "            # Filter faces based on min size\n",
    "            size = min(face.detection.location.width,face.detection.location.height)\n",
    "            if size < options.min_size:\n",
    "                continue\n",
    "    \n",
    "            if video_header_flag:\n",
    "                csv_header = ['source','frame','detect_id','type','score','x','y','w','h']\n",
    "                if len(face.landmarks) > 0:\n",
    "                    for each_lpt in face.landmarks:\n",
    "                        pt_id_label = each_lpt.landmark_id\n",
    "                        xpt_label = pt_id_label + '_x'\n",
    "                        ypt_label = pt_id_label + '_y'\n",
    "                        csv_header.append(pt_id_label)\n",
    "                        csv_header.append(xpt_label)\n",
    "                        csv_header.append(ypt_label)\n",
    "                file_identifier.writerow(csv_header)\n",
    "                video_header_flag = False\n",
    "            \n",
    "            csv_eachline = [face.source,\n",
    "                            face.frame,\n",
    "                            i,\n",
    "                            face.detection.detection_class,\n",
    "                            face.detection.score,\n",
    "                            face.detection.location.x,\n",
    "                            face.detection.location.y,\n",
    "                            face.detection.location.width,\n",
    "                            face.detection.location.height]\n",
    "\n",
    "            if len(face.landmarks) > 0:\n",
    "                for each_lpt in face.landmarks:\n",
    "                    pt_id_label = each_lpt.landmark_id\n",
    "                    xpt_label = each_lpt.location.x\n",
    "                    ypt_label = each_lpt.location.y\n",
    "                    csv_eachline.append(pt_id_label)\n",
    "                    csv_eachline.append(xpt_label)\n",
    "                    csv_eachline.append(ypt_label)\n",
    "                    \n",
    "            file_identifier.writerow(csv_eachline)\n",
    "                \n",
    "            if options.detect_log:\n",
    "                detect_log_dir = os.path.join(options.results_root_path, options.detect_log, media_type, base_name)\n",
    "                frame_id = face.frame\n",
    "                if not os.path.exists(detect_log_dir):\n",
    "                    os.makedirs(detect_log_dir, exist_ok=True)\n",
    "                \n",
    "                rect = pt.rect_proto2pv(face.detection.location)\n",
    "                if dimg is None:\n",
    "                    dimg = pv.Image(im[:,:,::-1])\n",
    "                dimg.annotateThickRect(rect)\n",
    "                dimg.annotateLabel(pv.Point(rect.x+5,rect.y+5),face.detection.detection_class)\n",
    "                dimg.annotateLabel(pv.Point(rect.x+5,rect.y+20),\n",
    "                                    \"Score: %0.4f\"%(face.detection.score,), color='yellow')\n",
    "                if len(face.landmarks) > 0:\n",
    "                    for each_lmark in face.landmarks:\n",
    "                            dimg.annotateCircle(pv.Point(each_lmark.location.x, each_lmark.location.y), \n",
    "                                                radius=3, color = 'green', fill='green')\n",
    "                \n",
    "            \n",
    "            if options.face_log:\n",
    "                face_log_dir = os.path.join(options.results_root_path, options.face_log, media_type, base_name)\n",
    "                if not os.path.exists(face_log_dir):\n",
    "                    os.makedirs(face_log_dir, exist_ok=True)\n",
    "                \n",
    "                rect = pt.rect_proto2pv(face.detection.location)\n",
    "                rect = rect.rescale(1.5)\n",
    "                affine = pv.AffineFromRect(rect,(128,128))\n",
    "                try:    \n",
    "                    pvim = pv.Image(im[:,:,::-1])\n",
    "                    view = affine(pvim)\n",
    "                    out_path = os.path.join(face_log_dir,os.path.basename(base_name)+\n",
    "                                            '_Frame_%06d'%(face.frame) +\n",
    "                                            '_face_%03d'%(face.detection.detection_id,)+ '.jpg')\n",
    "                    if len(face.landmarks) > 0:\n",
    "                        for each_lmark in face.landmarks:\n",
    "                            transformed_lmarks = affine(pv.Point(each_lmark.location.x, each_lmark.location.y))\n",
    "                            view.annotateCircle(transformed_lmarks, radius=3, color = 'green', fill='green')\n",
    "                    view.asAnnotated().save(out_path)\n",
    "                    #print('Saving face:',out_path)\n",
    "                except:\n",
    "                    print(\"WARNING: Image not processed correctly:\",face.source)\n",
    "            i += 1\n",
    "        \n",
    "        if options.detect_log and detect_log_dir is not None:\n",
    "            dimg.asAnnotated().save(os.path.join(detect_log_dir,\n",
    "                                                 os.path.basename(base_name) + '_Frame_%06d'%(frame_id) + '.jpg'))\n",
    "        return False\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(each_video, doptions):\n",
    "    \"\"\"\n",
    "    Looping through each frame in a video and making client calls to the service\n",
    "    \"\"\"\n",
    "    detect_queue = []\n",
    "    #Read Video\n",
    "    video = pv.Video(each_video)\n",
    "    videoname, ext = os.path.splitext(os.path.basename(each_video))\n",
    "    fid = open(os.path.join(doptions.results_root_path,\n",
    "                            doptions.csvfiles_log, 'video', videoname + '.csv') , 'w')\n",
    "    video_detections_csv = csv.writer(fid)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for frame_id, each_frame in enumerate(video):\n",
    "\n",
    "        each_frame = each_frame.asOpenCV2()[:,:,::-1]#convert to opencv and then bgrtorgb\n",
    "        if doptions.max_size is not None:\n",
    "            each_frame = preprocessImage(each_frame, doptions)\n",
    "        results = face_client.detect(each_frame, best=doptions.best, \n",
    "                             threshold=doptions.detect_thresh, \n",
    "                             min_size=doptions.min_size,\n",
    "                             run_async=True, \n",
    "                             source=each_video, \n",
    "                             frame=frame_id + 1)\n",
    "        detect_queue.append([each_frame, results, doptions, video_detections_csv, 'video'])\n",
    "        detect_queue = list(filter(process_video_detections, detect_queue))\n",
    "\n",
    "        \n",
    "    while len(detect_queue):\n",
    "        detect_queue = list(filter(process_video_detections,detect_queue))\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(start_time, end_time, end_time - start_time)\n",
    "    print(\"Processed %d frames in %0.3f seconds: %f images/second\"%(frame_id+1,end_time - start_time,\n",
    "                                                                    (frame_id+1)/(end_time - start_time)))\n",
    "    \n",
    "    fid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through video list \n",
    "video_header_flag = None\n",
    "if not os.path.isdir(os.path.join(detection_options.results_root_path, detection_options.csvfiles_log, 'video')):\n",
    "    os.makedirs(os.path.join(detection_options.results_root_path, detection_options.csvfiles_log, 'video'))\n",
    "for vid_num, each_video in enumerate(video_list):\n",
    "    global video_header_flag \n",
    "    video_header_flag = True\n",
    "    process_videos(each_video, detection_options)\n",
    "    \n",
    "    if detection_options.detect_log is not None:\n",
    "        from glob import glob\n",
    "        idir = os.path.join(detection_options.results_root_path, detection_options.detect_log, \n",
    "                            'video', os.path.splitext(os.path.basename(each_video))[0] )\n",
    "        bbox_imgs = glob(os.path.join(idir,'*'))\n",
    "        print('Number of Frames with Detection : ', len(bbox_imgs))\n",
    "        for each_img_path in bbox_imgs:\n",
    "            print(each_img_path)\n",
    "            img = mpimg.imread(each_img_path)\n",
    "            plt.imshow(img)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read images from detect_log dir and convert it to a video and play it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
